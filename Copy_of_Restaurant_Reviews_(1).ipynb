{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jA5jjOHd5Lh"
      },
      "source": [
        "# ðŸ˜€/ðŸ˜ž Sentiment Analysis on Restaurant Reviews using NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priprPfMeOT3"
      },
      "source": [
        "*This note book builds an end-to-end classification model uisng NLP, Naive Bayes.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT24x1tId_zu"
      },
      "source": [
        "### 1.Problem\n",
        "\n",
        "Identify whether the reviews of a restaurant made by customers are postive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K72F6rJSgZPo"
      },
      "source": [
        "### 2.Data\n",
        "The data is collected from"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmaurYglhLSr"
      },
      "source": [
        "### 3.Evaluation\n",
        "Evaluation is done on basis of metrics like accuracy score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55361qW6hbtL"
      },
      "source": [
        "### 4.Features\n",
        "Some information about data:\n",
        "* Entire data is in text format, so its better to use NLP for developing our ML models.\n",
        "* Overall, we have 17000 reviews (combining both the datasets).\n",
        "* We have two labels(columns) namely:\n",
        "  * Review : which is the review itself.\n",
        "  * Liked : which tells whether the review is positive or negative.\n",
        "    * 1 - for positive\n",
        "    * 0 - for negative\n",
        "* Text language is english."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j80oPjBslDPv"
      },
      "source": [
        "## Connect Google Drive to Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "GbofT8Inmn6G",
        "outputId": "8dc786a8-42ee-43e6-dd95-f1c47a62e733"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-91874b305a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 125\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OsUrEwnmy6E"
      },
      "source": [
        "## Get your workspace ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUEFaLJqnT7Z"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "# Other imports would be imported at specific necessary cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKWnwLmTnih1"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "We will do the pre-processing in three steps:\n",
        "  * Process data in dataset 1\n",
        "  * Process data in dataset 2\n",
        "  * Combine both the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLYnQOQvjduU"
      },
      "source": [
        "Preprocessing Dataset 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKwAfEVajwkb"
      },
      "source": [
        "## Clean the data\n",
        "  * Remove all the special characters\n",
        "  * Change all the characters to lower case\n",
        "  * Tokenize each reviews into words\n",
        "  * Remove stopwords\n",
        "  * Perform Stemming / Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0GZFQI17gpN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb3sjvZZ73U8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boIhCjI58d-B"
      },
      "outputs": [],
      "source": [
        "# load the final dataset into a pandas dataframe\n",
        "df1 = pd.read_csv(\"./drive/My Drive/MLstart/financial-sentiment-analysis/giftv1/model/dfAll.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ztt18t_8YCY"
      },
      "outputs": [],
      "source": [
        "df = df1[[\"Review\",\"Sentiment\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHfJVpLj_YYk"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNL-For7-HAo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'Sentiment'.\n",
        "df['Sentiment']= label_encoder.fit_transform(df['Sentiment'])\n",
        "\n",
        "df['Sentiment'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XiR1vQ0-7iD"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPdZzt5EWkUY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# df.to_csv(\"./drive/My Drive/MLstart/financial-sentiment-analysis/giftv1/dfc.csv\", index = False)\n",
        "# files.download('./drive/My Drive/MLstart/financial-sentiment-analysis/giftv1/dfc.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Ago4Qk8nol"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsFJul9wjZbO"
      },
      "outputs": [],
      "source": [
        "# Import other stopwords and PortetStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Function for cleaning the reviews\n",
        "def clean_text(text):\n",
        "\n",
        "  # cleaning special characters from reviews\n",
        "  review = re.sub(pattern='[^a-zA-Z]', repl=' ', string=text)\n",
        "\n",
        "  # converting each review into lowercase\n",
        "  review = review.lower()\n",
        "\n",
        "  # tokenizing the review into words\n",
        "  review_words = review.split()\n",
        "\n",
        "  # removing the stop words\n",
        "  review_words = [word for word in review_words if word not in set(stopwords.words('english'))]\n",
        "\n",
        "  # stemming the words\n",
        "  review_words = [ps.stem(word) for word in review_words]\n",
        "\n",
        "  # joining the stemmed words\n",
        "  review = ' '.join(review_words)\n",
        "\n",
        "  return review\n",
        "\n",
        "\n",
        "# clean text data\n",
        "df[\"Review_cleaned\"] = df[\"Review\"].apply(lambda x: clean_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew6qKBttlwZT"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E19hdLEuhfu"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PYdEf34xdAD"
      },
      "source": [
        "We use vader, which uses lexicon of words to find which ones are positive and negative. It takes into account the context of the sentences and determine the below four scores.\n",
        "  * a neutrality score\n",
        "  * a positivity score\n",
        "  * a negativity score\n",
        "  * an overall score that summarizes the previous scores\n",
        "\n",
        "We will integrate these four values as features in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1-dcSZruqgA"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "df['sentiments_tmp'] = df[\"Review\"].apply(lambda x: sia.polarity_scores(x))\n",
        "df = pd.concat([df.drop(['sentiments_tmp'], axis=1), df['sentiments_tmp'].apply(pd.Series)], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UelxwLnSwm1M"
      },
      "source": [
        "Next we add some metrics to each review\n",
        "* Number of characters in each review\n",
        "* Number of words in each review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1Ml46jkzFOc"
      },
      "outputs": [],
      "source": [
        "# add number of characters column\n",
        "df[\"nb_chars\"] = df['Review'].apply(lambda x : len(x))\n",
        "\n",
        "# add number of words column\n",
        "df[\"nb_words\"] = df[\"Review\"].apply(lambda x : len(x.split(\" \")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N_xyyMEzts3"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DqMWT7eXxtC"
      },
      "source": [
        "Extracting vector representations for each review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vWWgLzTaNdl"
      },
      "outputs": [],
      "source": [
        "# create doc2Vec vector columns\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df[\"Review_cleaned\"].apply(lambda x: x.split(\" \")))]\n",
        "\n",
        "# train doc2vec model with our text data\n",
        "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
        "\n",
        "# transform each document into a vector data\n",
        "doc2vec_df = df[\"Review_cleaned\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
        "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
        "df = pd.concat([df, doc2vec_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8douXsKjDGGI"
      },
      "outputs": [],
      "source": [
        " # save the trained doc2vec model\n",
        "pickle.dump(model, open('./drive/My Drive/MLstart/financial-sentiment-analysis/giftv1/model/doc2vec-model.pkl', 'wb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULEgk8aZduJg"
      },
      "source": [
        "Add TF-IDF values for every word and every document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOrn_wa7dTLe"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(min_df=10)\n",
        "tfidf_result = tfidf.fit_transform(df['Review_cleaned']).toarray()\n",
        "tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf.get_feature_names())\n",
        "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
        "tfidf_df.index = df.index\n",
        "df = pd.concat([df, tfidf_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58gGfmobXxiF"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syLUPjXXgVmK"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhkj_K8rCmP6"
      },
      "outputs": [],
      "source": [
        "# save the trained tfidf model\n",
        "pickle.dump(model, open('./drive/My Drive/MLstart/financial-sentiment-analysis/giftv1/model/tfidf-model.pkl', 'wb'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA9eX94hgqv1"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a15R1mO6gu4w"
      },
      "source": [
        "### Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x81vN36Mg9bm",
        "outputId": "106fece5-ab08-4b0c-ffb8-103531749efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-45e03f9ad80d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# print wordcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mshow_wordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# wordcloud function\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        background_color = 'white',\n",
        "        max_words = 500,\n",
        "        max_font_size = 40,\n",
        "        scale = 3,\n",
        "        random_state = 12\n",
        "    ).generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize = (10, 10))\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize = 20)\n",
        "        fig.subplots_adjust(top = 2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()\n",
        "\n",
        "# print wordcloud\n",
        "show_wordcloud(df[\"Review\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elDlglpt9u1f"
      },
      "source": [
        "### Create test and train sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVXrTYOe-jcm"
      },
      "outputs": [],
      "source": [
        "# feature selection\n",
        "label = 'Sentiment'\n",
        "ignore_columns = [label, 'Tweets', 'Review_cleaned']\n",
        "\n",
        "features = [c for c in df.columns if not c in ignore_columns]\n",
        "\n",
        "# split the data into test and train sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = df[features], df[label]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43E-umOK_AP2"
      },
      "outputs": [],
      "source": [
        "(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dyM9NaL_DfA"
      },
      "outputs": [],
      "source": [
        "(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPS4gpOX_QMP"
      },
      "source": [
        "### Model Building\n",
        "\n",
        "Here we are using Naive Bayes classifier model, it works on probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6shDeT-S_ki2"
      },
      "outputs": [],
      "source": [
        "# import Random Forest Classifier from sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# create an instance for MultinomialNB\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the data to the model\n",
        "rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Goxxj3hLrTh2"
      },
      "outputs": [],
      "source": [
        "## Feature Importance\n",
        "\n",
        "feature_importances_df = pd.DataFrame({'feature':features, 'importance':rf.feature_importances_}).sort_values('importance', ascending=False)\n",
        "feature_importances_df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAaJZKGMAOuT"
      },
      "source": [
        "### Predict test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gENfdElOAmgA"
      },
      "outputs": [],
      "source": [
        "y_preds = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxiifQNOAsbC"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReuQu9OzA1FM"
      },
      "outputs": [],
      "source": [
        "# Calculate Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_preds)\n",
        "recall = recall_score(y_test, y_preds)\n",
        "precision = precision_score(y_test, y_preds)\n",
        "\n",
        "print(\"///---Scores---///\")\n",
        "print('Accuracy Score is : {}%'.format(round(accuracy*100,2)))\n",
        "print('Precision Score is : {}'.format(round(precision,2)))\n",
        "print('Recall Score is : {}'.format(round(recall,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuEUOZOhxmVG"
      },
      "source": [
        "Calculate evaluation metrics with cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zZpFo--xxt6"
      },
      "outputs": [],
      "source": [
        "# import cross_val_score from sklearn\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create a new classifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "# Cross-Validated Accuracy\n",
        "accuracy_cv = np.mean(cross_val_score(classifier,\n",
        "                                      X,\n",
        "                                      y,\n",
        "                                      cv=5,\n",
        "                                      scoring='accuracy'))\n",
        "\n",
        "# Cross-Validated Precision\n",
        "precision_cv = np.mean(cross_val_score(classifier,\n",
        "                                      X,\n",
        "                                      y,\n",
        "                                      cv=5,\n",
        "                                      scoring='precision'))\n",
        "\n",
        "# Cross-Validated Recall\n",
        "recall_cv = np.mean(cross_val_score(classifier,\n",
        "                                      X,\n",
        "                                      y,\n",
        "                                      cv=5,\n",
        "                                      scoring='recall'))\n",
        "\n",
        "print(\"///---Cross-Validated Scores---///\")\n",
        "print('Accuracy Score is : {}%'.format(round(accuracy_cv*100,2)))\n",
        "print('Precision Score is : {}'.format(round(precision_cv,2)))\n",
        "print('Recall Score is : {}'.format(round(recall_cv,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VUAGNJWCWdu"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Eb0pwsfDc5f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_preds)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLx52dRuDsed"
      },
      "outputs": [],
      "source": [
        "# Plotting confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(cm, annot=True, cmap=\"YlGnBu\", xticklabels=['Neagtive', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Actual Values')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TFr4VjuFrGW"
      },
      "source": [
        "### Hyperparameter Tuning using RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqjmFb2bIYnn"
      },
      "outputs": [],
      "source": [
        "# # import RandomizedSearchCV from sklearn\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# # Create a hyperparameter grid for our classifier\n",
        "# param_grid = {'alpha':np.arange(0.1,1.1,0.1)}\n",
        "\n",
        "# # Setup hyperparameter search for RandomForestClassifier classifier\n",
        "# rs = RandomizedSearchCV(RandomForestClassifier(),\n",
        "#                         param_distributions=param_grid,\n",
        "#                         cv=5,\n",
        "#                         n_iter=10,\n",
        "#                         verbose=True,\n",
        "#                         random_state=0)\n",
        "\n",
        "# # Fit the hyperparameter search\n",
        "# rs.fit(X_train ,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-2yY9q4J46m"
      },
      "outputs": [],
      "source": [
        "# rs.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLGKTTzHAJUw"
      },
      "outputs": [],
      "source": [
        "# rs_y_preds = rs.predict(X_test)\n",
        "# print(classification_report(y_test, rs_y_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY_GRnujKAg1"
      },
      "outputs": [],
      "source": [
        "# rs.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQbCj-6RKGxC"
      },
      "outputs": [],
      "source": [
        "# Re-train the classifier with best params derived from Hyperparameter Tuning.\n",
        "classifier = RandomForestClassifier(random_state=42)\n",
        "classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0522YNSWBiKI"
      },
      "outputs": [],
      "source": [
        "# save the trained classifier to use it predictions anywhere else\n",
        "# pickle.dump(classifier, open('/content/drive/My Drive/Restaurant Reviews/restaurant-sentiment-rf-model.pkl', 'wb'))\n",
        "pickle.dump(classifier, open('./drive/My Drive/MLstart/financial-sentiment-analysis/v/restaurant-sentiment-rf-model.pkl', 'wb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blKMsmLvPXhr"
      },
      "source": [
        "## Making predictions for real-world reviews\n",
        "\n",
        "Create a function which will take a sample review as input and give prediction as output using our trained classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnwWc3lWX8a8"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjXEjPtqXmk9"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(sample_review):\n",
        "\n",
        "  sample_review_cleand = re.sub(pattern='[^a-zA-Z]', repl=' ', string=sample_review)\n",
        "  sample_review_cleand = sample_review_cleand.lower()\n",
        "  sample_review_words = sample_review_cleand.split()\n",
        "  sample_review_words = [word for word in sample_review_words if word not in set(stopwords.words('english'))]\n",
        "  ps = PorterStemmer()\n",
        "  sample_review_words = [ps.stem(word) for word in sample_review_words]\n",
        "  sample_review_cleand = ' '.join(sample_review_words)\n",
        "  sample_df = pd.DataFrame()\n",
        "  sample_df['Review'] = [sample_review]\n",
        "  sample_df['Review_cleaned'] = [sample_review_cleand]\n",
        "  sample_df['sentiments_tmp'] = sample_df[\"Review\"].apply(lambda x: sia.polarity_scores(x))\n",
        "  sample_df = pd.concat([sample_df.drop(['sentiments_tmp'], axis=1), sample_df['sentiments_tmp'].apply(pd.Series)], axis=1)\n",
        "  sample_df[\"nb_chars\"] = sample_df['Review'].apply(lambda x : len(x))\n",
        "  sample_df[\"nb_words\"] = sample_df[\"Review\"].apply(lambda x : len(x.split(\" \")))\n",
        "  sample_doc2vec_df = sample_df[\"Review_cleaned\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
        "  sample_doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in sample_doc2vec_df.columns]\n",
        "  sample_df = pd.concat([sample_df, sample_doc2vec_df], axis=1)\n",
        "  sample_tfidf_result = tfidf.transform(sample_df['Review_cleaned']).toarray()\n",
        "  sample_tfidf_df = pd.DataFrame(sample_tfidf_result, columns=tfidf.get_feature_names())\n",
        "  sample_tfidf_df.columns = [\"word_\" + str(x) for x in sample_tfidf_df.columns]\n",
        "  sample_tfidf_df.index = sample_df.index\n",
        "  sample_df = pd.concat([sample_df, sample_tfidf_df], axis=1)\n",
        "  sample_features = [c for c in sample_df.columns if not c in ['Review', 'Review_cleaned']]\n",
        "\n",
        "  return classifier.predict(sample_df[sample_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAQQQsbpSWJt"
      },
      "outputs": [],
      "source": [
        "# Predicting values\n",
        "sample_review = 'The poor banking skills'\n",
        "print(type(predict_sentiment(sample_review)[0]))\n",
        "\n",
        "if predict_sentiment(sample_review) == 1:\n",
        "  print('This is a POSITIVE REVIEW')\n",
        "else:\n",
        "  print('This is NEGATIVE REVIEW')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbOtG9q2ShFy"
      },
      "outputs": [],
      "source": [
        "# Predicting values\n",
        "sample_review = 'Food was pretty bad and the service was very slow.'\n",
        "\n",
        "if predict_sentiment(sample_review) == 1:\n",
        "  print('This is a POSITIVE REVIEW')\n",
        "else:\n",
        "  print('This is NEGATIVE REVIEW')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-MafEoDTk6l"
      },
      "outputs": [],
      "source": [
        "# Predicting values\n",
        "sample_review = 'The food was absolutely wonderful, from preparation to presentation, very pleasing.'\n",
        "\n",
        "if predict_sentiment(sample_review) == 1:\n",
        "  print('This is a POSITIVE REVIEW')\n",
        "else:\n",
        "  print('This is NEGATIVE REVIEW')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbWN2nGuKyLg"
      },
      "outputs": [],
      "source": [
        "# Predicting values\n",
        "sample_review = 'I am not Happy with food .'\n",
        "\n",
        "if predict_sentiment(sample_review) == 1:\n",
        "  print('This is a POSITIVE REVIEW')\n",
        "else:\n",
        "  print('This is NEGATIVE REVIEW')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUf-GZJIVz8F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}